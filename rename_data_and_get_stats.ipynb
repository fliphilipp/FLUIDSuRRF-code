{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41a3ff-14b9-44ca-bca6-1cd5076ba524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "os.environ[\"GDAL_DATA\"] = \"/home/parndt/anaconda3/envs/geo_py37/share/gdal\"\n",
    "os.environ[\"PROJ_LIB\"] = \"/home/parndt/anaconda3/envs/geo_py37/share/proj\"\n",
    "import h5py\n",
    "import math\n",
    "import datetime\n",
    "import traceback\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from cmcrameri import cm as cmc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# from icelakes.utilities import convert_time_to_string\n",
    "from IPython.display import Image, display\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from lakeanalysis.utils import dictobj, get_quality_summary, convert_time_to_string, read_melt_lake_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb4872-a68c-4849-b401-65de40825f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = ''\n",
    "out_data_dir = 'detection_out_data/'\n",
    "searchfor = '.h5'\n",
    "searchdir = base_dir + out_data_dir\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f)]\n",
    "filelist.sort()\n",
    "print('There are %i data files.' % len(filelist))\n",
    "\n",
    "out_plot_dir = 'detection_out_plot/'\n",
    "searchfor_img = '.jpg'\n",
    "searchdir = base_dir + out_plot_dir\n",
    "filelist_img = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor_img in f)]\n",
    "filelist_img.sort()\n",
    "print('There are %i plot files.' % len(filelist_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85577e21-0f5e-4a91-a38a-192df45cd52b",
   "metadata": {},
   "source": [
    "# Rename files \n",
    "## make sure this works before actually re-naming files, just run once!!\n",
    "This also moves the plots to the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380d4b9-c676-4696-8fc0-473811205638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(filelist, dry_run=True, print_n_names=5): \n",
    "\n",
    "    if dry_run: \n",
    "        files_ = filelist[:print_n_names].copy()\n",
    "    else:\n",
    "        files_ = filelist.copy()\n",
    "\n",
    "    num_missing_data = 0\n",
    "    for i,fn in enumerate(files_):\n",
    "        \n",
    "        print('reading file %5i / %5i' % (i+1, len(files_)), end='\\r')\n",
    "\n",
    "        try:\n",
    "            with h5py.File(fn, 'r+') as f:\n",
    "                lake_quality = f['properties']['lake_quality'][()]\n",
    "                detection_quality = f['properties']['detection_quality'][()]\n",
    "                surf_elev = f['properties']['surface_elevation'][()]\n",
    "                depth = f['depth_data']['depth'][()]\n",
    "                conf = f['depth_data']['conf'][()]\n",
    "                xatc = f['depth_data']['xatc'][()]\n",
    "                fitbed = f['depth_data']['h_fit_bed'][()]\n",
    "                xtent = f['properties']['surface_extent_detection'][()]\n",
    "                isdepth = (xatc >= xtent[0]) & (xatc <= xtent[-1]) & (fitbed < (surf_elev-0.5)) & (depth < 50)\n",
    "                max_depth = np.percentile(depth[isdepth], 95)\n",
    "                quality_summary = get_quality_summary(detection_quality, lake_quality)\n",
    "                if 'max_depth' in f['properties'].keys():\n",
    "                    del f['properties/max_depth']\n",
    "                if 'quality_summary' in f['properties'].keys():\n",
    "                    del f['properties/quality_summary']\n",
    "                dset = f.create_dataset('properties/max_depth', data=max_depth)\n",
    "                dset = f.create_dataset('properties/quality_summary', data=quality_summary)\n",
    "        except: \n",
    "            # print('file misses data:', fn)\n",
    "            # traceback.print_exc()\n",
    "            detection_quality = 0.0\n",
    "            lake_quality = 0.0\n",
    "            num_missing_data += 1\n",
    "                \n",
    "        quality_summary = get_quality_summary(detection_quality, lake_quality)\n",
    "        filenameonly = fn[fn.find('lake_'):]\n",
    "        pathtofile = fn[:fn.find('lake_')]\n",
    "        parm_list = filenameonly.split('_')\n",
    "        parm_list[1] = '%08i' % np.round((100 - quality_summary)*100000)\n",
    "        if parm_list[2].isnumeric():\n",
    "            del parm_list[2]\n",
    "        file_name = '_'.join(parm_list)\n",
    "        newpath = pathtofile + file_name\n",
    "        plot_fn_old = fn.replace('.h5', '.jpg')\n",
    "        plot_fn_new = newpath.replace('.h5', '.jpg')\n",
    "\n",
    "        if dry_run:\n",
    "            print(detection_quality, lake_quality, quality_summary)\n",
    "            print('fn:', fn)\n",
    "            print('newpath:', newpath)\n",
    "            print('plot_fn_old:', plot_fn_old)\n",
    "            print('plot_fn_new:', plot_fn_new)\n",
    "            print('')\n",
    "        else:\n",
    "            os.rename(fn, newpath)\n",
    "            if os.path.isfile(plot_fn_old):\n",
    "                os.rename(plot_fn_old, plot_fn_new)\n",
    "            \n",
    "    print('')\n",
    "    print('lakes with missing data: %i' % num_missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88876d7c-ca6a-4d40-a973-13546ab08677",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files(filelist, dry_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb427b9a-9b0f-4907-89d1-38640e8b3fb7",
   "metadata": {},
   "source": [
    "# ONLY RUN THIS LINE ONCE, AFTER CHECKING IF IT GIVES THE RIGHT RESULTS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b1797-3bc9-4f2d-85da-e8fa74864327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS LINE ONCE, AFTER CHECKING IF IT GIVES THE RIGHT RESULTS!!!\n",
    "# rename_files(filelist, dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18d1c9-4ef6-40e1-949c-008798ce9cf6",
   "metadata": {},
   "source": [
    "# get stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e765e6-c9f4-4fc3-9047-638224ccf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfor = '.h5'\n",
    "searchdir = base_dir + out_data_dir\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f)]\n",
    "filelist.sort()\n",
    "print('There are %i files.' % len(filelist))\n",
    "zerolakes = [f for f in filelist if 'lake_10000000_' in f]\n",
    "print('There are %i files with zero quality.' % len(zerolakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1681a-0510-41a4-8928-3005bfed7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing_data = 0\n",
    "\n",
    "for i,fn in enumerate(filelist):\n",
    "    print('reading file %5i / %5i' % (i+1, len(filelist)), end='\\r')\n",
    "\n",
    "    try:\n",
    "        with h5py.File(fn, 'r') as f:\n",
    "            ice_sheet = f['properties']['ice_sheet'][()].decode('utf-8')\n",
    "            melt_season = f['properties']['melt_season'][()].decode('utf-8')\n",
    "            polygon_name = f['properties']['polygon_name'][()].decode('utf-8')\n",
    "            max_depth = f['properties']['max_depth'][()]\n",
    "            length_water_surfaces = f['properties']['length_water_surfaces'][()]\n",
    "            surface_elevation = f['properties']['surface_elevation'][()]\n",
    "            n_photons_where_water = f['properties']['n_photons_where_water'][()]\n",
    "            lon = f['properties']['lon'][()]\n",
    "            lat = f['properties']['lat'][()]\n",
    "            lon_min = f['properties']['lon_min'][()]\n",
    "            lon_max = f['properties']['lon_max'][()]\n",
    "            lat_min = f['properties']['lat_min'][()]\n",
    "            lat_max = f['properties']['lat_max'][()]\n",
    "            cycle_number = f['properties']['cycle_number'][()]\n",
    "            rgt = f['properties']['rgt'][()]\n",
    "            gtx = f['properties']['gtx'][()].decode('utf-8')\n",
    "            beam_strength = f['properties']['beam_strength'][()].decode('utf-8')\n",
    "            beam_number = f['properties']['beam_number'][()]\n",
    "            granule_id = f['properties']['granule_id'][()].decode('utf-8')\n",
    "            lake_id = f['properties']['lake_id'][()].decode('utf-8')\n",
    "            \n",
    "            date_time = convert_time_to_string(np.mean(f['mframe_data']['dt'][()]))\n",
    "            lake_quality = f['properties']['lake_quality'][()]\n",
    "            detection_quality = f['properties']['detection_quality'][()]\n",
    "            quality_summary = f['properties']['quality_summary'][()]\n",
    "            \n",
    "        basin = polygon_name.replace('simplified_', '')\n",
    "        file_name = fn\n",
    "    \n",
    "        datadict = {\n",
    "            'ice_sheet': ice_sheet,\n",
    "            'melt_season': melt_season,\n",
    "            'basin': basin,\n",
    "            'quality_summary': quality_summary,\n",
    "            'max_depth': max_depth,\n",
    "            'length_water_surfaces': length_water_surfaces,\n",
    "            'surface_elevation': surface_elevation,\n",
    "            'n_photons_where_water': n_photons_where_water,\n",
    "            'lon': lon,\n",
    "            'lat': lat,\n",
    "            'date_time': date_time,\n",
    "            'lon_min': lon_min,\n",
    "            'lon_max': lon_max,\n",
    "            'lat_min': lat_min,\n",
    "            'lat_max': lat_max,\n",
    "            'cycle_number': cycle_number,\n",
    "            'rgt': rgt,\n",
    "            'gtx': gtx,\n",
    "            'beam_strength': beam_strength,\n",
    "            'beam_number': beam_number,\n",
    "            'detection_quality': detection_quality,\n",
    "            'lake_quality': lake_quality,\n",
    "            'granule_id': granule_id,\n",
    "            'lake_id': lake_id,\n",
    "            'file_name': file_name\n",
    "        }\n",
    "    \n",
    "        if i == 0: \n",
    "            df = pd.DataFrame(datadict, index=[0])\n",
    "        else:\n",
    "            df.loc[i] = datadict.values()\n",
    "\n",
    "    except:\n",
    "        num_missing_data += 1\n",
    "\n",
    "print('\\nNumber of lakes with missing data: %i' % num_missing_data)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeicelakes-env",
   "language": "python",
   "name": "eeicelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

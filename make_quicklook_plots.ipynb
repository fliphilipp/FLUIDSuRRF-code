{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21781074-081b-45b8-bbf0-8aad2ab31c1f",
   "metadata": {},
   "source": [
    "# Produce \"Quicklook\" Imagery Plots for output lake files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567882e9-11d5-44a1-aeef-799d4ee5e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "os.environ[\"GDAL_DATA\"] = \"/Users/parndt/anaconda3/envs/eeicelakes-env/share/gdal\"\n",
    "os.environ[\"PROJ_LIB\"] = \"/Users/parndt/anaconda3/envs/eeicelakes-env/share/proj\"\n",
    "os.environ[\"PROJ_DATA\"] = \"/Users/parndt/anaconda3/envs/eeicelakes-env/share/proj\"\n",
    "import ee\n",
    "import h5py\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import traceback\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from datetime import datetime \n",
    "import rasterio as rio\n",
    "from rasterio import plot as rioplot\n",
    "from rasterio import warp\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from cmcrameri import cm as cmc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from lakeanalysis.utils import dictobj, convert_time_to_string, read_melt_lake_h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f147b0-d2ed-4e44-bafb-513f6fa8fdef",
   "metadata": {},
   "source": [
    "## the functions for imagery retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd6ed8f-7842-49d2-bb1b-2a13ad8393dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_bands = ['B4', 'B3', 'B2']\n",
    "viz_bands_rename = ['R', 'G', 'B']\n",
    "min_sun_elevation = 20\n",
    "\n",
    "#####################################################################\n",
    "def get_cloudfree_image_collection(area_of_interest, date_time, days_buffer, max_cloud_scene=50, max_cloud_mask=20):\n",
    "    \n",
    "    datetime_requested = datetime.strptime(date_time, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    start_date = (datetime_requested - timedelta(days=days_buffer)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_date = (datetime_requested + timedelta(days=days_buffer)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    # print('Looking for images from %s to %s' % (start_date, end_date), end=' ')\n",
    "    \n",
    "    def get_sentinel2_collection(area_of_interest, start_date, end_date):\n",
    "        sentinel2_collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "                                .filterBounds(area_of_interest)\n",
    "                                .filterDate(start_date, end_date)\n",
    "                                .filterMetadata('MEAN_SOLAR_ZENITH_ANGLE', 'less_than', ee.Number(90).subtract(min_sun_elevation)))\n",
    "    \n",
    "        s2cloudless_collection = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "                                  .filterBounds(area_of_interest)\n",
    "                                  .filterDate(start_date, end_date))\n",
    "    \n",
    "        return (ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "            'primary': sentinel2_collection,\n",
    "            'secondary': s2cloudless_collection,\n",
    "            'condition': ee.Filter.equals(**{\n",
    "                'leftField': 'system:index',\n",
    "                'rightField': 'system:index'\n",
    "            })\n",
    "        })))\n",
    "    \n",
    "    def add_cloud_bands_S2(image):\n",
    "        cloud = ee.Image(image.get('s2cloudless')).select('probability').rename('cloudScore')\n",
    "        scene_id = image.get('PRODUCT_ID')\n",
    "        return image.addBands(cloud).set('scene_id', scene_id)\n",
    "    \n",
    "    def mask_S2(img):\n",
    "        mask = img.select('cloudScore').lt(max_cloud_mask)\n",
    "        return img.updateMask(mask)\n",
    "    \n",
    "    def normalize_brightness_S2(image):\n",
    "        return image.addBands(image.select(viz_bands).rename(viz_bands_rename))\n",
    "    \n",
    "    def get_landsat_collection(area_of_interest, start_date, end_date):\n",
    "        L8T1 = ee.ImageCollection('LANDSAT/LC08/C02/T1_TOA')\n",
    "        L8T2 = ee.ImageCollection('LANDSAT/LC08/C02/T2_TOA')\n",
    "        L9T1 = ee.ImageCollection('LANDSAT/LC09/C02/T1_TOA')\n",
    "        L9T2 = ee.ImageCollection('LANDSAT/LC09/C02/T2_TOA')\n",
    "        return (L8T1.merge(L8T2).merge(L9T2).merge(L9T2)\n",
    "                .filterBounds(area_of_interest)\n",
    "                .filterDate(start_date, end_date)\n",
    "                   .filterMetadata('SUN_ELEVATION', 'greater_than', min_sun_elevation))\n",
    "    \n",
    "    def landsat_cloud_score(image):\n",
    "        cloud = ee.Algorithms.Landsat.simpleCloudScore(image).select('cloud').rename('cloudScore')\n",
    "        scene_id = image.get('LANDSAT_PRODUCT_ID')\n",
    "        return image.addBands(cloud).set('scene_id', scene_id)\n",
    "    \n",
    "    def mask_landsat(img):\n",
    "        mask = img.select('cloudScore').lt(max_cloud_mask)\n",
    "        return img.updateMask(mask)\n",
    "    \n",
    "    def normalize_brightness_LST(image):\n",
    "        return image.addBands(image.select(viz_bands).rename(viz_bands_rename))\n",
    "    \n",
    "    def get_sentinel2_cloud_collection(area_of_interest, start_date, end_date, max_cloud_scene=50, max_cloud_mask=20):\n",
    "        \n",
    "        def set_cloudiness(img, aoi=area_of_interest):\n",
    "            cloudprob = img.select(['cloudScore']).reduceRegion(reducer=ee.Reducer.mean(), \n",
    "                                                                 geometry=aoi, \n",
    "                                                                 bestEffort=True, \n",
    "                                                                 maxPixels=1e6)\n",
    "            return img.set('ground_track_cloud_prob', cloudprob.get('cloudScore'))\n",
    "        \n",
    "        return (get_sentinel2_collection(area_of_interest, start_date, end_date)\n",
    "                         .map(add_cloud_bands_S2)\n",
    "                         .map(set_cloudiness)\n",
    "                         # .filter(ee.Filter.lt('ground_track_cloud_prob', max_cloud_scene))\n",
    "                         .map(mask_S2)\n",
    "                         .map(normalize_brightness_S2)\n",
    "               )\n",
    "    \n",
    "    def get_landsat_cloud_collection(area_of_interest, start_date, end_date, max_cloud_scene=50, max_cloud_mask=20):\n",
    "        \n",
    "        def set_cloudiness(img, aoi=area_of_interest):\n",
    "            cloudprob = img.select(['cloudScore']).reduceRegion(reducer=ee.Reducer.mean(), \n",
    "                                                                 geometry=aoi, \n",
    "                                                                 bestEffort=True, \n",
    "                                                                 maxPixels=1e6)\n",
    "            return img.set('ground_track_cloud_prob', cloudprob.get('cloudScore'))\n",
    "        \n",
    "        return (get_landsat_collection(area_of_interest, start_date, end_date)\n",
    "                         .map(landsat_cloud_score)\n",
    "                         .map(set_cloudiness)\n",
    "                         # .filter(ee.Filter.lt('ground_track_cloud_prob', max_cloud_scene))\n",
    "                         .map(mask_landsat)\n",
    "                         .map(normalize_brightness_LST)\n",
    "               )\n",
    "    \n",
    "    # def clipToROI(img):\n",
    "    #     return img.clip(area_of_interest)\n",
    "\n",
    "    S2_collection = get_sentinel2_cloud_collection(area_of_interest, start_date, end_date, max_cloud_scene, max_cloud_mask)\n",
    "    LST_collection = get_landsat_cloud_collection(area_of_interest, start_date, end_date, max_cloud_scene, max_cloud_mask)\n",
    "\n",
    "    return S2_collection.merge(LST_collection)\n",
    "\n",
    "#####################################################################\n",
    "def download_imagery(fn, lk, gt, imagery_filename, days_buffer=3, max_cloud_scene=50, max_cloud_mask=20, gamma_value=1.8, \n",
    "                     buffer_factor=1.2, crs_out='EPSG:3031', n_imgs_mosaic=7, scale_out=10, mosaic_method='mosaic'):\n",
    "\n",
    "    lake_mean_delta_time = lk.mframe_data.dt.mean()\n",
    "    ATLAS_SDP_epoch_datetime = datetime(2018, 1, 1, tzinfo=timezone.utc) # 2018-01-01:T00.00.00.000000 UTC, from ATL03 data dictionary \n",
    "    ATLAS_SDP_epoch_timestamp = datetime.timestamp(ATLAS_SDP_epoch_datetime)\n",
    "    lake_mean_timestamp = ATLAS_SDP_epoch_timestamp + lake_mean_delta_time\n",
    "    lake_mean_datetime = datetime.fromtimestamp(lake_mean_timestamp, tz=timezone.utc)\n",
    "    time_format_out = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    is2time = datetime.strftime(lake_mean_datetime, time_format_out)\n",
    "\n",
    "    # get the bounding box\n",
    "    lon_rng = gt.lon.max() - gt.lon.min()\n",
    "    lat_rng = gt.lat.max() - gt.lat.min()\n",
    "    fac = 0.25\n",
    "    bbox = [gt.lon.min()-fac*lon_rng, gt.lat.min()-fac*lat_rng, gt.lon.max()+fac*lon_rng, gt.lat.max()+fac*lat_rng]\n",
    "    poly = [(bbox[x[0]], bbox[x[1]]) for x in [(0,1), (2,1), (2,3), (0,3), (0,1)]]\n",
    "    roi = ee.Geometry.Polygon(poly)\n",
    "\n",
    "    # get the earth engine collection\n",
    "    collection_size = 0\n",
    "    if days_buffer > 200:\n",
    "        days_buffer = 200\n",
    "    increment_days = days_buffer\n",
    "    while (collection_size<n_imgs_mosaic) & (days_buffer <= 200):\n",
    "\n",
    "        cloudfree_collection = get_cloudfree_image_collection(area_of_interest=roi, \n",
    "                                                    date_time=is2time, \n",
    "                                                    days_buffer=days_buffer, \n",
    "                                                    max_cloud_scene=max_cloud_scene, \n",
    "                                                    max_cloud_mask=max_cloud_mask)\n",
    "\n",
    "        cloudfree_collection = cloudfree_collection.filter(ee.Filter.lt('ground_track_cloud_prob', max_cloud_scene))\n",
    "        collection_size = cloudfree_collection.size().getInfo()\n",
    "        days_buffer += increment_days\n",
    "    \n",
    "    # get the time difference between ICESat-2 and Sentinel-2 and sort by it \n",
    "    # is2time = lk.date_time\n",
    "                         \n",
    "    def set_time_difference(img, is2time=lake_mean_timestamp):\n",
    "        timediff = ee.Date(lake_mean_timestamp*1000).difference(img.get('system:time_start'), 'second').abs()\n",
    "        return img.set('timediff', timediff)\n",
    "    cloudfree_collection = cloudfree_collection.map(set_time_difference).sort('timediff')\n",
    "\n",
    "    # create a region around the ground track over which to download data\n",
    "    lon_center = gt.lon.mean()\n",
    "    lat_center = gt.lat.mean()\n",
    "    gt_length = gt.x10.max() - gt.x10.min()\n",
    "    point_of_interest = ee.Geometry.Point(lon_center, lat_center)\n",
    "    region_of_interest = point_of_interest.buffer(np.nanmax((gt_length*0.5*buffer_factor,500)))\n",
    "\n",
    "    prod_id = 'no imagery'\n",
    "    if collection_size > 0:\n",
    "        \n",
    "        # stretch the color values \n",
    "        def color_stretch(image):\n",
    "            percentiles = image.select(viz_bands_rename).reduceRegion(**{\n",
    "                'reducer': ee.Reducer.percentile(**{'percentiles': [3, 97], 'outputNames': ['lower', 'upper']}),\n",
    "                'geometry': region_of_interest,\n",
    "                'scale': 30,\n",
    "                'maxPixels': 1e9,\n",
    "                'bestEffort': True\n",
    "            })\n",
    "            lower = percentiles.select(['.*_lower']).values().reduce(ee.Reducer.min())\n",
    "            upper = percentiles.select(['.*_upper']).values().reduce(ee.Reducer.max())\n",
    "            return image.select(viz_bands_rename).unitScale(lower, upper).clamp(0,1).resample('bilinear').reproject(**{'crs': crs_out,'scale': scale_out})\n",
    "        \n",
    "        # limit mosaic to first n images after sorting by timediff\n",
    "        selectedImage = cloudfree_collection.first()\n",
    "        # to_mosaic = cloudfree_collection.limit(n_imgs_mosaic,'timediff',True).map(color_stretch)\n",
    "        to_mosaic = cloudfree_collection.limit(n_imgs_mosaic).map(color_stretch)\n",
    "        # select the first image for info\n",
    "        \n",
    "        if mosaic_method == 'mean':\n",
    "            rgb = to_mosaic.mean()\n",
    "        elif mosaic_method == 'median':\n",
    "            rgb = to_mosaic.median()\n",
    "        else:\n",
    "            # mosaic() puts last image in collection on top, so sort by timediff in descending order (the False value)\n",
    "            # to_mosaic = to_mosaic.sort('timediff', True)\n",
    "            print('mosaic with closest image on top')\n",
    "            # info = to_mosaic.getInfo()\n",
    "            # for f in info['features']:\n",
    "            #     print(f['properties']['timediff'], f['properties']['scene_id'])\n",
    "            to_mosaic = to_mosaic.sort('timediff', False)\n",
    "            rgb = to_mosaic.mosaic()\n",
    "            \n",
    "        rgb_gamma = rgb.pow(1/gamma_value)\n",
    "        rgb8bit = rgb_gamma.clamp(0,1).multiply(255).uint8()\n",
    "        \n",
    "        # from the selected image get some stats: product id, cloud probability and time difference from icesat-2\n",
    "        prod_id = selectedImage.get('scene_id').getInfo()\n",
    "        cld_prb = selectedImage.get('ground_track_cloud_prob').getInfo()\n",
    "        s2_timestamp = selectedImage.get('system:time_start').getInfo()\n",
    "        s2datetime = datetime.fromtimestamp(s2_timestamp/1000, tz=timezone.utc)\n",
    "        s2datestr = datetime.strftime(s2datetime, 'time_format_out')\n",
    "        is2datetime = lake_mean_datetime\n",
    "        timediff = s2datetime - is2datetime\n",
    "        timediff_str = '%s' % (timediff)\n",
    "        \n",
    "        # get the download URL and download the selected image\n",
    "        success = False\n",
    "        tries = 0\n",
    "        while (success == False) & (tries <= 10):\n",
    "            try:\n",
    "                downloadURL = rgb8bit.getDownloadUrl({'name': 'mySatelliteImage',\n",
    "                                                          'crs': crs_out,\n",
    "                                                          'scale': scale_out,\n",
    "                                                          'region': region_of_interest,\n",
    "                                                          'filePerBand': False,\n",
    "                                                          'format': 'GEO_TIFF'})\n",
    "        \n",
    "                response = requests.get(downloadURL)\n",
    "                with open(imagery_filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "        \n",
    "                # print('--> Downloaded the 8-bit RGB image as %s.' % imagery_filename)\n",
    "                success = True\n",
    "                tries += 1\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                scale_out *= 2\n",
    "                # print('-> download unsuccessful, increasing scale to %.1f...' % scale)\n",
    "                success = False\n",
    "                tries += 1\n",
    "\n",
    "    return prod_id, timediff_str\n",
    "            \n",
    "#####################################################################\n",
    "def plot_imagery(fn, days_buffer=5, max_cloud_scene=50, max_cloud_mask=20, xlm=[None, None], ylm=[None, None], gamma_value=1.8, imagery_filename=None,\n",
    "                 re_download=True, ax=None, buffer_factor=1.5, crs_out='EPSG:3031', n_imgs_mosaic=7, scale_out=10, mosaic_method='mosaic'):\n",
    "                     \n",
    "    lk = dictobj(read_melt_lake_h5(fn))\n",
    "    time_format_out = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    lake_mean_delta_time = lk.mframe_data.dt.mean()\n",
    "    ATLAS_SDP_epoch_datetime = datetime(2018, 1, 1, tzinfo=timezone.utc) # 2018-01-01:T00.00.00.000000 UTC, from ATL03 data dictionary \n",
    "    ATLAS_SDP_epoch_timestamp = datetime.timestamp(ATLAS_SDP_epoch_datetime)\n",
    "    lake_mean_timestamp = ATLAS_SDP_epoch_timestamp + lake_mean_delta_time\n",
    "    lake_mean_datetime = datetime.fromtimestamp(lake_mean_timestamp, tz=timezone.utc)\n",
    "    lake_mean_time_string = datetime.strftime(lake_mean_datetime, time_format_out)\n",
    "    lk.date_time = lake_mean_time_string\n",
    "                     \n",
    "    df = lk.photon_data.copy()\n",
    "    if not xlm[0]:\n",
    "        xlm[0] = df.xatc.min()\n",
    "    if not xlm[1]:\n",
    "        xlm[1] = df.xatc.max()\n",
    "    if not ylm[0]:\n",
    "        ylm[0] = lk.surface_elevation-2*lk.max_depth\n",
    "    if not ylm[1]:\n",
    "        ylm[1] = lk.surface_elevation+lk.max_depth\n",
    "    if not imagery_filename:\n",
    "        imagery_filename = 'imagery' + fn[fn.rfind('/'):].replace('.h5','.tif')\n",
    "    \n",
    "    \n",
    "    df = df[(df.xatc >= xlm[0]) & (df.xatc <= xlm[1]) & (df.h >= ylm[0]) & (df.h <= ylm[1])].reset_index(drop=True).copy()\n",
    "    x_off = np.min(df.xatc)\n",
    "    df.xatc -= x_off\n",
    "    \n",
    "    dfd = lk.depth_data.copy()\n",
    "    dfd.xatc -= x_off\n",
    "\n",
    "    # get the ground track\n",
    "    df['x10'] = np.round(df.xatc, -1)\n",
    "    gt = df.groupby(by='x10')[['lat', 'lon']].median().reset_index()\n",
    "    lon_center = gt.lon.mean()\n",
    "    lat_center = gt.lat.mean()\n",
    "                     \n",
    "    try:\n",
    "        prod_id = 'no imagery'\n",
    "        if ((not os.path.isfile(imagery_filename)) or re_download) and ('modis' not in imagery_filename):\n",
    "            prod_id, dt_str = download_imagery(fn=fn, lk=lk, gt=gt, imagery_filename=imagery_filename, days_buffer=days_buffer, \n",
    "                             max_cloud_scene=max_cloud_scene, max_cloud_mask=max_cloud_mask, gamma_value=gamma_value, \n",
    "                             buffer_factor=buffer_factor, crs_out=crs_out, n_imgs_mosaic=n_imgs_mosaic, scale_out=scale_out,\n",
    "                             mosaic_method=mosaic_method)\n",
    "        \n",
    "        try:\n",
    "            myImage = rio.open(imagery_filename)\n",
    "            \n",
    "            # make the figure\n",
    "            if not ax:\n",
    "                fig, ax = plt.subplots(figsize=[6,6])\n",
    "            \n",
    "            rioplot.show(myImage, ax=ax)\n",
    "            ax.axis('off')\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    \n",
    "        try:\n",
    "            dt_str = dt_str.split('.')[0]\n",
    "            if dt_str[0] != '-':\n",
    "                dt_str = '+' + dt_str\n",
    "            text = 'closest image time difference: %s\\n%s' % (dt_str, prod_id)\n",
    "            ax.text(0.01, 0.01, text, fontsize=6, ha='left', va='bottom', transform=ax.transAxes,\n",
    "               bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.1,rounding_size=0.3', lw=0), zorder=3000)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    \n",
    "        try:\n",
    "            ximg, yimg = warp.transform(src_crs='epsg:4326', dst_crs=myImage.crs, xs=np.array(gt.lon), ys=np.array(gt.lat))\n",
    "            if 'modis' in imagery_filename:\n",
    "                xrng = ximg[-1] - ximg[0]\n",
    "                yrng = yimg[-1] - yimg[0]\n",
    "                fac = 30\n",
    "                # print('using saved modis image')\n",
    "                ax.plot([ximg[-1]+fac*xrng,ximg[0]-fac*xrng], [yimg[-1]+fac*yrng, yimg[0]-fac*yrng], 'k:', lw=1)\n",
    "                ax.annotate('', xy=(ximg[-1]+fac*xrng, yimg[-1]+fac*yrng), xytext=(ximg[0]-fac*xrng, yimg[0]-fac*yrng),\n",
    "                                 arrowprops=dict(width=0, lw=0, headwidth=5, headlength=5, color='k'),zorder=1000)\n",
    "                # ax.plot(ximg, yimg, 'r-', lw=1, zorder=5000)\n",
    "            else:\n",
    "                # print('plotting ground track')\n",
    "                ax.annotate('', xy=(ximg[-1], yimg[-1]), xytext=(ximg[0], yimg[0]),\n",
    "                                 arrowprops=dict(width=0.7, headwidth=5, headlength=5, color='k'),zorder=1000)\n",
    "                try:\n",
    "                    isdepth = dfd.depth>0\n",
    "                    bed = dfd.h_fit_bed\n",
    "                    bed[~isdepth] = np.nan\n",
    "                    bed[(dfd.depth>2) & (dfd.conf < 0.3)] = np.nan\n",
    "                    surf = np.ones_like(dfd.xatc) * lk.surface_elevation\n",
    "                    surf[~isdepth] = np.nan\n",
    "                    xatc_surf = np.array(dfd.xatc)[~np.isnan(surf)]\n",
    "                    lon_bed = np.array(dfd.lon)\n",
    "                    lat_bed = np.array(dfd.lat)\n",
    "                    lon_bed[(np.isnan(surf)) & (np.isnan(bed))] = np.nan\n",
    "                    lat_bed[(np.isnan(surf)) & (np.isnan(bed))] = np.nan\n",
    "                    xb, yb = warp.transform(src_crs='epsg:4326', dst_crs=myImage.crs, xs=lon_bed, ys=lat_bed)\n",
    "                    ax.plot(xb, yb, 'r-', lw=1, zorder=5000)\n",
    "                except:\n",
    "                    traceback.print_exc()\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        if not ax:\n",
    "            fig.tight_layout(pad=0)\n",
    "    \n",
    "        return myImage, lon_center, lat_center\n",
    "    except: \n",
    "        return None, lon_center, lat_center\n",
    "        traceback.print_exc()\n",
    "\n",
    "                     \n",
    "#####################################################################\n",
    "def plotIS2(fn, ax=None, xlm=[None, None], ylm=[None,None], cmap=cmc.lapaz_r, name='ICESat-2 data'):\n",
    "    lk = dictobj(read_melt_lake_h5(fn))\n",
    "    time_format_out = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    lake_mean_delta_time = lk.mframe_data.dt.mean()\n",
    "    ATLAS_SDP_epoch_datetime = datetime(2018, 1, 1, tzinfo=timezone.utc) # 2018-01-01:T00.00.00.000000 UTC, from ATL03 data dictionary \n",
    "    ATLAS_SDP_epoch_timestamp = datetime.timestamp(ATLAS_SDP_epoch_datetime)\n",
    "    lake_mean_timestamp = ATLAS_SDP_epoch_timestamp + lake_mean_delta_time\n",
    "    lake_mean_datetime = datetime.fromtimestamp(lake_mean_timestamp, tz=timezone.utc)\n",
    "    lake_mean_time_string = datetime.strftime(lake_mean_datetime, time_format_out)\n",
    "    lk.date_time = lake_mean_time_string\n",
    "    df = lk.photon_data.copy()\n",
    "    dfd = lk.depth_data.copy()\n",
    "\n",
    "    try:\n",
    "        isdepth = dfd.depth>0\n",
    "        bed = dfd.h_fit_bed\n",
    "        bed[~isdepth] = np.nan\n",
    "        bed[(dfd.depth>2) & (dfd.conf < 0.3)] = np.nan\n",
    "        surf = np.ones_like(dfd.xatc) * lk.surface_elevation\n",
    "        surf[~isdepth] = np.nan\n",
    "        surf_only = surf[~np.isnan(surf)]\n",
    "        bed_only = bed[(~np.isnan(surf)) & (~np.isnan(bed))]\n",
    "        xatc_surf = np.array(dfd.xatc)[~np.isnan(surf)]\n",
    "        xatc_bed = np.array(dfd.xatc)[(~np.isnan(surf)) & (~np.isnan(bed))]\n",
    "        \n",
    "        # make the figure\n",
    "        if not ax:\n",
    "            fig, ax = plt.subplots(figsize=[8,5])\n",
    "    \n",
    "        df['is_afterpulse']= df.prob_afterpulse > np.random.uniform(0,1,len(df))\n",
    "        if not cmap:\n",
    "            # ax.scatter(df.xatc, df.h, s=1, c='k')\n",
    "            ax.scatter(df.xatc[~df.is_afterpulse], df.h[~df.is_afterpulse], s=1, c='k')\n",
    "        else:\n",
    "            ax.scatter(df.xatc[~df.is_afterpulse], df.h[~df.is_afterpulse], s=1, c=df.snr, cmap=cmap)\n",
    "        \n",
    "        ax.scatter(dfd.xatc[isdepth], dfd.h_fit_bed[isdepth], s=4, color='r', alpha=dfd.conf[isdepth])\n",
    "        ax.plot(dfd.xatc, dfd.h_fit_bed, color='gray', lw=0.5)\n",
    "        \n",
    "        ax.plot(dfd.xatc, bed, color='r', lw=1)\n",
    "        ax.plot(dfd.xatc, surf, color='C0', lw=1)\n",
    "    except:\n",
    "        ax.scatter(df.xatc, df.h, s=1, c='k')\n",
    "        ax.plot(dfd.xatc, dfd.h_fit_bed, 'r-')\n",
    "        ax.plot(dfd.xatc, np.ones_like(dfd.xatc) * lk.surface_elevation, 'b-')\n",
    "\n",
    "    # add the length of surface\n",
    "    try:\n",
    "        arr_y = lk.surface_elevation+lk.max_depth*0.25\n",
    "        x_start = np.min(xatc_surf)\n",
    "        x_end = np.max(xatc_surf)\n",
    "        x_mid = (x_end + x_start) / 2\n",
    "        len_surf_m = np.floor((x_end-x_start)/100)*100\n",
    "        len_surf_km = len_surf_m/1000\n",
    "        arr_x1 = x_mid - len_surf_m / 2\n",
    "        arr_x2 = x_mid + len_surf_m / 2\n",
    "        ax.annotate('', xy=(arr_x1, arr_y), xytext=(arr_x2, arr_y),\n",
    "                             arrowprops=dict(width=0.7, headwidth=5, headlength=5, color='C0'),zorder=1000)\n",
    "        ax.annotate('', xy=(arr_x2, arr_y), xytext=(arr_x1, arr_y),\n",
    "                             arrowprops=dict(width=0.7, headwidth=5, headlength=5, color='C0'),zorder=1000)\n",
    "        ax.text(x_mid, arr_y, '%.1f km' % len_surf_km, fontsize=12, ha='center', va='bottom', color='C0', fontweight='bold',\n",
    "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.2,rounding_size=0.5', lw=0))\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # add the max depth\n",
    "    try:\n",
    "        if np.sum(np.isnan(bed)) < len(bed):\n",
    "            y_low = np.nanmin(bed)\n",
    "            if y_low < (lk.surface_elevation - 2 * lk.max_depth):\n",
    "                bed[bed < lk.surface_elevation - 2 * lk.max_depth] = np.nan\n",
    "                if np.sum(np.isnan(bed)) < len(bed):\n",
    "                    y_low = np.nanmin(bed)\n",
    "            if np.sum(np.isnan(bed)) < len(bed):\n",
    "                y_up = lk.surface_elevation\n",
    "                arr_x = xatc_bed[np.argmin(bed_only)]\n",
    "                xlm = (df.xatc.min(), df.xatc.max())\n",
    "                arr_x = xlm[0] - 0.0* (xlm[1] - xlm[0])\n",
    "                y_len = y_up - y_low\n",
    "                y_mid = (y_up + y_low) / 2\n",
    "                arr_len = y_len\n",
    "                arr_y1 = y_mid + arr_len / 2\n",
    "                arr_y2 = y_mid - arr_len / 2\n",
    "                ref_index = 1.33\n",
    "                dep_round = np.round(y_len / ref_index, 1)\n",
    "                ax.annotate('', xy=(arr_x, arr_y2), xytext=(arr_x, arr_y1),\n",
    "                                     arrowprops=dict(width=0.7, headwidth=5, headlength=5, color='r'),zorder=1000)\n",
    "                ax.annotate('', xy=(arr_x, arr_y1), xytext=(arr_x, arr_y2),\n",
    "                                     arrowprops=dict(width=0.7, headwidth=5, headlength=5, color='r'),zorder=1000)\n",
    "                ax.text(arr_x, y_mid, '%.1f m' % dep_round, fontsize=12, ha='right', va='center', color='r', fontweight='bold',\n",
    "                        bbox=dict(facecolor='white', alpha=0.8, lw=0, boxstyle='round,pad=0.2,rounding_size=0.5'), rotation=90)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # add the title\n",
    "    try:\n",
    "        datestr = datetime.strftime(datetime.strptime(lk.date_time,'%Y-%m-%dT%H:%M:%SZ'), '%d %B %Y %H:%M:%S UTC')\n",
    "        sheet = lk.ice_sheet\n",
    "        region = lk.polygon_filename.split('_')[-1].replace('.geojson', '')\n",
    "        if sheet == 'AIS':\n",
    "            region = region + ' (%s)' % lk.polygon_filename.split('_')[-2]\n",
    "        latstr = lk.lat_str[:-1] + '°' + lk.lat_str[-1]\n",
    "        lonstr = lk.lon_str[:-1] + '°' + lk.lon_str[-1]\n",
    "        description = '%s, %s - %s (%s, %s, %.1fm)\\n%s %s (%s)' % (datestr, sheet, region, latstr, lonstr, lk.surface_elevation, lk.granule_id, lk.gtx, lk.beam_strength)\n",
    "        \n",
    "        ax.text(0.5, 1.0, description, fontsize=8, ha='center', va='top', transform=ax.transAxes,\n",
    "               bbox=dict(facecolor='white', alpha=0.9, boxstyle='round,pad=0.2,rounding_size=0.5', lw=0), zorder=3000)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # add the lake quality\n",
    "    try:\n",
    "        thiscol = 'g' if lk.lake_quality > 0 else 'r'\n",
    "        ax.text(0.97, 0.03, 'SuRRF quality score: %.2f' % lk.lake_quality, fontsize=12, ha='right', va='bottom', color=thiscol, \n",
    "                fontweight='bold', transform=ax.transAxes, \n",
    "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.2,rounding_size=0.5', lw=0))\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    try:\n",
    "        xlm = (df.xatc.min(), df.xatc.max())\n",
    "        dp = lk.max_depth\n",
    "        if dp < 2.0:\n",
    "            dp = 2.0\n",
    "        ylm = (lk.surface_elevation-dp*2, lk.surface_elevation+dp)\n",
    "        ax.set_xlim(xlm)\n",
    "        ax.set_ylim(ylm)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    ax.axis('off')\n",
    "\n",
    "    \n",
    "#####################################################################\n",
    "def plot_IS2_imagery(fn, axes=None, xlm=[None,None], ylm=[None,None], cmap=None, days_buffer=5, max_cloud_scene=50, max_cloud_mask=50,\n",
    "                     gamma_value=1.8, imagery_filename=None, re_download=True, img_aspect=3/2, name='ICESat-2 data',\n",
    "                     return_fig=False, n_imgs_mosaic=7, scale_out=10, mosaic_method='mosaic'):\n",
    "\n",
    "    if not axes:\n",
    "        fig = plt.figure(figsize=[12,6], dpi=80)\n",
    "        gs = fig.add_gridspec(1,3)\n",
    "        axp = [fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1:])]\n",
    "    else:\n",
    "        axp = axes\n",
    "        \n",
    "    ax = axp[1]\n",
    "    try:\n",
    "        plotIS2(fn=fn, ax=ax, xlm=xlm, ylm=ylm, cmap=cmap, name=name)\n",
    "    except:\n",
    "        ax.text(0.5, 0.5, 'plotting error', fontsize=12, ha='center', va='center', transform=ax.transAxes)\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    ax = axp[0]\n",
    "    try:\n",
    "        with h5py.File(fn) as f:\n",
    "            icesheet = f['properties']['ice_sheet'][()].decode('utf-8')\n",
    "        crs_out = 'EPSG:3413' if icesheet=='GrIS' else 'EPSG:3031'\n",
    "        center_lon, center_lat = 0, 0\n",
    "        img, center_lon, center_lat = plot_imagery(fn=fn, days_buffer=days_buffer, max_cloud_scene=max_cloud_scene, \n",
    "            max_cloud_mask=max_cloud_mask, xlm=xlm, ylm=ylm, gamma_value=gamma_value, imagery_filename=imagery_filename, \n",
    "            re_download=re_download, ax=ax, crs_out=crs_out, n_imgs_mosaic=n_imgs_mosaic, scale_out=scale_out,\n",
    "            mosaic_method=mosaic_method)\n",
    "        \n",
    "        if img:        \n",
    "            if imagery_filename:\n",
    "                if 'modis' in imagery_filename:\n",
    "                    center_x, center_y = warp.transform(src_crs='epsg:4326', dst_crs=img.crs, xs=[center_lon], ys=[center_lat])\n",
    "                    center_x = center_x[0]\n",
    "                    center_y = center_y[0]\n",
    "                    rng = 220000\n",
    "                    if img_aspect > 1:\n",
    "                        ax.set_xlim(center_x - 0.5*rng/img_aspect, center_x + 0.5*rng/img_aspect)\n",
    "                        ax.set_ylim(center_y - 0.5*rng, center_y + 0.5*rng)\n",
    "                    if img_aspect < 1:\n",
    "                        ax.set_xlim(center_x - 0.5*rng, center_x + 0.5*rng)\n",
    "                        ax.set_ylim(center_y - 0.5*rng*img_aspect, center_y + 0.5*rng*img_aspect)\n",
    "                    \n",
    "            if (img_aspect > 1): \n",
    "                h_rng = img.bounds.top - img.bounds.bottom\n",
    "                cntr = (img.bounds.right + img.bounds.left) / 2\n",
    "                ax.set_xlim(cntr-0.5*h_rng/img_aspect, cntr+0.5*h_rng/img_aspect)\n",
    "            elif img_aspect < 1: \n",
    "                w_rng = img.bounds.right - img.bounds.left\n",
    "                cntr = (img.bounds.top + img.bounds.bottom) / 2\n",
    "                ax.set_ylim(cntr-0.5*w_rng*img_aspect, cntr+0.5*w_rng/img_aspect)\n",
    "    except:\n",
    "        ax.text(0.5, 0.5, 'plotting error', fontsize=12, ha='center', va='center', transform=ax.transAxes)\n",
    "        traceback.print_exc()\n",
    "            \n",
    "    \n",
    "    if not axes:\n",
    "        fig.tight_layout(pad=1, h_pad=0, w_pad=0)\n",
    "        if not name:\n",
    "            name = 'zzz' + lk.polygon_filename.split('_')[-1].replace('.geojson', '')\n",
    "        outname = 'figplots/' + name.replace(' ', '') + fn[fn.rfind('/')+1:].replace('.h5','.jpg')\n",
    "        fig.savefig(outname, dpi=300)\n",
    "\n",
    "    if return_fig:\n",
    "        plt.close(fig)\n",
    "        return center_lon, center_lat, fig\n",
    "    else:\n",
    "        return center_lon, center_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313eb3f-1a6a-4c23-90a1-6da1b65e277a",
   "metadata": {},
   "source": [
    "## rename files with more concise names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c2d2a-0eed-4b32-9fcd-b9c16ac94e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchdir = 'data/lakes_data/'\n",
    "searchfor = '.h5'\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f)]\n",
    "filelist.sort()\n",
    "listlength = len(filelist)\n",
    "print('number .h5:', listlength)\n",
    "searchfor_img = '.jpg'\n",
    "filelist_img = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor_img in f)]\n",
    "filelist_img.sort()\n",
    "listlength_img = len(filelist_img)\n",
    "print('number .jpg:', listlength_img)\n",
    "\n",
    "df = pd.DataFrame({'fn_in': filelist, 'fn_out': ' '})\n",
    "def get_lake_quality(x):\n",
    "    with h5py.File(x.fn_in) as f:\n",
    "        return f['properties']['lake_quality'][()]\n",
    "def get_lake_sheet(x):\n",
    "    with h5py.File(x.fn_in) as f:\n",
    "        return f['properties']['ice_sheet'][()].decode('utf-8')\n",
    "def get_melt_season(x):\n",
    "    with h5py.File(x.fn_in) as f:\n",
    "        return f['properties']['melt_season'][()].decode('utf-8')\n",
    "def get_time(x):\n",
    "    with h5py.File(x.fn_in) as f:\n",
    "        return f['properties']['time_utc'][()].decode('utf-8')\n",
    "\n",
    "df['lake_quality'] = df.apply(get_lake_quality, axis=1)\n",
    "df['ice_sheet'] = df.apply(get_lake_sheet, axis=1)\n",
    "df['melt_season'] = df.apply(get_melt_season, axis=1)\n",
    "df['time_utc'] = df.apply(get_time, axis=1)\n",
    "df = df.sort_values(by='lake_quality', ascending=False).reset_index(drop=True)\n",
    "df['granule_gtx'] = df.apply(lambda x: x.fn_in[x.fn_in.find('ATL03_'):x.fn_in.rfind('_')].upper(), axis=1)\n",
    "df['ilake'] = 0\n",
    "for gt in np.unique(df.granule_gtx):\n",
    "    selectgt = df.granule_gtx == gt\n",
    "    df.loc[selectgt, 'ilake'] = np.arange(1,np.sum(selectgt)+1)\n",
    "print('unique gts:', len(np.unique(df.granule_gtx)))\n",
    "df['lakeid'] = df.apply(lambda x: '%s_%04d' % (x.granule_gtx, x.ilake), axis=1)\n",
    "print('unique ids:', len(np.unique(df.lakeid)))\n",
    "df['qual_str'] = df.apply(lambda x: '%07d' % int(np.round(1e6 - np.clip(x.lake_quality,0,1000)*1e3)), axis=1)\n",
    "df['fn_out'] = df.apply(lambda x: 'data/lakes_data/lake_%s_%s.h5' % (x.qual_str, x.lakeid), axis=1)\n",
    "df.apply(lambda x: os.rename(x.fn_in, x.fn_out), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d5282-709e-443e-a8ee-840e730c4f30",
   "metadata": {},
   "source": [
    "## make a list of all data files / check for image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a4254a4-31d6-4ca6-9e01-a683ce555ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number .h5: 1249\n",
      "number .jpg: 1249\n"
     ]
    }
   ],
   "source": [
    "searchdir = 'data/lakes_data/'\n",
    "searchfor = '.h5'\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f)]\n",
    "filelist.sort()\n",
    "listlength = len(filelist)\n",
    "print('number .h5:', listlength)\n",
    "searchfor_img = '.jpg'\n",
    "filelist_img = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor_img in f)]\n",
    "filelist_img.sort()\n",
    "listlength_img = len(filelist_img)\n",
    "print('number .jpg:', listlength_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abaa4e2-9efc-4876-a1d1-f63f50ff9149",
   "metadata": {},
   "source": [
    "## clean up file structure and rename datasets to be more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3a9778b-9f07-4d65-8296-9a01433d2430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " data/lakes_data/lake_0921898_ATL03_20190818034635_07800403_006_02_GT3L_0001.h5\n",
      "group: depth_data/\n",
      "  - dataset: bathymetry_confidence\n",
      "  - dataset: lakebed_fit_elevation_meters\n",
      "  - dataset: lat\n",
      "  - dataset: lon\n",
      "  - dataset: surface_fit_elevation_meters\n",
      "  - dataset: water_depth_meters\n",
      "  - dataset: x_along_track_meters\n",
      "group: fluid_bathymetry_peaks/\n",
      "  - dataset: elevation_meters\n",
      "  - dataset: peak_prominence\n",
      "  - dataset: x_along_track_meters\n",
      "group: mframe_data/\n",
      "  - dataset: delta_time\n",
      "  - dataset: density_ratio_1\n",
      "  - dataset: density_ratio_2\n",
      "  - dataset: density_ratio_3\n",
      "  - dataset: density_ratio_4\n",
      "  - dataset: major_frame_id\n",
      "  - dataset: passes_bathymetry_check\n",
      "  - dataset: passes_flatness_check\n",
      "  - dataset: photon_density_peak_elevation\n",
      "  - dataset: q_1_number_peaks\n",
      "  - dataset: q_2_prominece\n",
      "  - dataset: q_3_elev_spread\n",
      "  - dataset: q_4_alignment\n",
      "  - dataset: q_s\n",
      "  - dataset: x_along_track_meters_end\n",
      "  - dataset: x_along_track_meters_start\n",
      "group: photon_data/\n",
      "  - dataset: afterpulse_probability\n",
      "  - dataset: fluid_signal_confidence\n",
      "  - dataset: geoid_elevation_meters\n",
      "  - dataset: lat\n",
      "  - dataset: lon\n",
      "  - dataset: photon_elevation_above_geoid_meters\n",
      "  - dataset: pulse_saturation_level\n",
      "  - dataset: x_along_track_meters\n",
      "group: properties/\n",
      "  - dataset: beam_number\n",
      "  - dataset: beam_strength\n",
      "  - dataset: cycle_number\n",
      "  - dataset: granule_id\n",
      "  - dataset: gtx\n",
      "  - dataset: ice_sheet\n",
      "  - dataset: lake_quality\n",
      "  - dataset: lat\n",
      "  - dataset: lon\n",
      "  - dataset: melt_season\n",
      "  - dataset: rgt\n",
      "  - dataset: sc_orient\n",
      "  - dataset: surface_elevation\n",
      "  - dataset: time_utc\n"
     ]
    }
   ],
   "source": [
    "for fn in filelist:\n",
    "    try:\n",
    "        with h5py.File(fn, 'r+') as f:\n",
    "\n",
    "            if 'detection_quality_info' in f.keys():\n",
    "                del f['detection_quality_info']\n",
    "\n",
    "            if 'detection_2nd_returns' in f.keys():\n",
    "                if 'fluid_bathymetry_peaks' not in f.keys():\n",
    "                    f['fluid_bathymetry_peaks'] = f['detection_2nd_returns']\n",
    "                del f['detection_2nd_returns']\n",
    "                \n",
    "            ###################################################################\n",
    "            grp = 'properties'\n",
    "            to_remove = [\n",
    "                'dead_time_meters', \n",
    "                'detection_quality',\n",
    "                'full_lat_extent_detection',\n",
    "                'lake_id',\n",
    "                'lat_max',\n",
    "                'lat_min',\n",
    "                'lat_str',\n",
    "                'lat_surface_extent_detection',\n",
    "                'len_subsegs',\n",
    "                'length_extent',\n",
    "                'length_water_surfaces',\n",
    "                'lon_max',\n",
    "                'lon_min',\n",
    "                'lon_str',\n",
    "                'main_peak',\n",
    "                'max_depth',\n",
    "                'mframe_end',\n",
    "                'mframe_start',\n",
    "                'n_photons_where_water',\n",
    "                'n_subsegs_per_mframe',\n",
    "                'oaurl',\n",
    "                'polygon_filename',\n",
    "                'polygon_name',\n",
    "                'quality_summary',\n",
    "                'surface_extent_detection',\n",
    "                'dead_time'\n",
    "            ]\n",
    "            for k in to_remove:\n",
    "                if k in f[grp].keys():\n",
    "                    del f[grp][k]\n",
    "                    \n",
    "            ###################################################################\n",
    "            grp = 'photon_data'\n",
    "            to_remove = [\n",
    "                'is_afterpulse', \n",
    "                'mframe',\n",
    "                'ph_id_pulse',\n",
    "                'prob_bed',\n",
    "                'prob_surf',\n",
    "                'sat_elev',\n",
    "            ]\n",
    "            to_rename =  {\n",
    "                'geoid': 'geoid_elevation_meters',\n",
    "                'h': 'photon_elevation_above_geoid_meters',\n",
    "                'xatc': 'x_along_track_meters',\n",
    "                'prob_afterpulse': 'afterpulse_probability',\n",
    "                'sat_ratio': 'pulse_saturation_level',\n",
    "                'snr': 'fluid_signal_confidence',\n",
    "            }\n",
    "            for k in to_remove:\n",
    "                if k in f[grp].keys():\n",
    "                    del f[grp][k]\n",
    "            if grp in f.keys():\n",
    "                for k,v in to_rename.items():\n",
    "                    if (k in f[grp].keys()) and (v not in f[grp].keys()):\n",
    "                        f[grp][v] = f[grp][k]\n",
    "                        del f[grp][k]\n",
    "                        \n",
    "            ###################################################################\n",
    "            grp = 'depth_data'\n",
    "            to_remove = [\n",
    "                'std_bed', \n",
    "                'std_surf'\n",
    "            ]\n",
    "            to_rename = {\n",
    "                'conf': 'bathymetry_confidence',\n",
    "                'depth': 'water_depth_meters',\n",
    "                'h_fit_bed': 'lakebed_fit_elevation_meters',\n",
    "                'h_fit_surf': 'surface_fit_elevation_meters',\n",
    "                'xatc': 'x_along_track_meters'\n",
    "            }\n",
    "            for k in to_remove:\n",
    "                if k in f[grp].keys():\n",
    "                    del f[grp][k]\n",
    "            if grp in f.keys():\n",
    "                for k,v in to_rename.items():\n",
    "                    if (k in f[grp].keys()) and (v not in f[grp].keys()):\n",
    "                        f[grp][v] = f[grp][k]\n",
    "                        del f[grp][k]\n",
    "                        \n",
    "            ###################################################################\n",
    "            grp = 'fluid_bathymetry_peaks'\n",
    "            to_rename =  {\n",
    "                'h': 'elevation_meters',\n",
    "                'prom': 'peak_prominence',\n",
    "                'xatc': 'x_along_track_meters'\n",
    "            }\n",
    "            if grp in f.keys():\n",
    "                for k,v in to_rename.items():\n",
    "                    if (k in f[grp].keys()) and (v not in f[grp].keys()):\n",
    "                        f[grp][v] = f[grp][k]\n",
    "                        del f[grp][k]\n",
    "\n",
    "            ###################################################################\n",
    "            grp = 'mframe_data'\n",
    "            to_remove = [\n",
    "                'xatc', \n",
    "                'n_phot',\n",
    "                'lat',\n",
    "                'lon',\n",
    "                'ratio_2nd_returns',\n",
    "            ]\n",
    "            to_rename = {\n",
    "                'dt': 'delta_time',\n",
    "                'is_flat': 'passes_flatness_check',\n",
    "                'lake_qual_pass': 'passes_bathymetry_check',\n",
    "                'mframe': 'major_frame_id',\n",
    "                'peak': 'photon_density_peak_elevation',\n",
    "                'xatc_max': 'x_along_track_meters_start',\n",
    "                'xatc_min': 'x_along_track_meters_end',\n",
    "                'snr_lower': 'density_ratio_1',\n",
    "                'snr_upper': 'density_ratio_2',\n",
    "                'snr_surf': 'density_ratio_3',\n",
    "                'snr_allabove': 'density_ratio_4',\n",
    "                'quality_summary': 'q_s',\n",
    "                'length_penalty': 'q_1_number_peaks',\n",
    "                'quality_secondreturns': 'q_2_prominece',\n",
    "                'range_penalty': 'q_3_elev_spread',\n",
    "                'alignment_penalty': 'q_4_alignment',\n",
    "                \n",
    "            }\n",
    "            for k in to_remove:\n",
    "                if k in f[grp].keys():\n",
    "                    del f[grp][k]\n",
    "            if grp in f.keys():\n",
    "                for k,v in to_rename.items():\n",
    "                    if (k in f[grp].keys()) and (v not in f[grp].keys()):\n",
    "                        f[grp][v] = f[grp][k]\n",
    "                        del f[grp][k]\n",
    "            \n",
    "            print('\\n', fn)\n",
    "            for k in f.keys():\n",
    "                print('group: ', k, '/', sep='')\n",
    "                for kk in f[k].keys():\n",
    "                    print('  - dataset: ' + kk)\n",
    "            \n",
    "    except:\n",
    "        print('\\n_______________________________________________________________________________')\n",
    "        print('WARNING: Changing lake file failed!')\n",
    "        print(fn)\n",
    "        print('\\n')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5b74f1-28be-4935-8922-285771e5d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################################################\n",
    "# # code for making sure it worked\n",
    "# def read_lake_file(fn):\n",
    "#     lakedict = {}\n",
    "#     with h5py.File(fn, 'r') as f:\n",
    "#         # metadata\n",
    "#         for key in f['properties'].keys(): \n",
    "#             lakedict[key] = f['properties'][key][()]\n",
    "#             if f['properties'][key].dtype == object:\n",
    "#                 lakedict[key] = lakedict[key].decode('utf-8')\n",
    "#         names = ['depth_data', 'photon_data', 'mframe_data', 'fluid_bathymetry_peaks']\n",
    "#         for name in names:\n",
    "#             d = {}\n",
    "#             for key in f[name].keys():\n",
    "#                 d[key] = f[name][key][()]\n",
    "#             lakedict[name] = pd.DataFrame(d)\n",
    "#     return lakedict\n",
    "\n",
    "# file_name = filelist[0]\n",
    "# lakedict = read_lake_file(file_name)\n",
    "# fig, ax = plt.subplots()\n",
    "# df_photon = lakedict['photon_data']\n",
    "# df_depth = lakedict['depth_data']\n",
    "# ax.scatter(df_photon.x_along_track_meters, df_photon.photon_elevation_above_geoid_meters, s=1, color='k')\n",
    "# ax.plot(df_depth.x_along_track_meters, df_depth.surface_fit_elevation_meters, color='b')\n",
    "# ax.plot(df_depth.x_along_track_meters, df_depth.lakebed_fit_elevation_meters, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc112f-40a1-4f07-8e88-a0c71aff7c29",
   "metadata": {},
   "source": [
    "## create all the quicklook plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557f4849-a30a-49c7-aac2-c7343a398bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[####################################################################################################]\n",
      "\n",
      "1249 / 1249 lakes processed (1249 this run)\n",
      "0 days, 1 hrs, 53 mins, 9 secs elapsed\n",
      "0 days, 0 hrs, 0 mins, 5 secs remaining\n",
      "current file: lake_1000000_ATL03_20210208190023_07141012_006_01_GT3R_0002_imagery.jpg\n"
     ]
    }
   ],
   "source": [
    "##### FOR METHODS DATA\n",
    "replace_existing = False # whether to make a new plot if a plot with the specified name already exists\n",
    "\n",
    "searchdir = 'data/lakes_data/'\n",
    "searchfor = '.h5'\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f)]\n",
    "filelist.sort()\n",
    "listlength = len(filelist)\n",
    "print('number .h5:', listlength)\n",
    "searchfor_img = '.jpg'\n",
    "filelist_img = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor_img in f)]\n",
    "filelist_img.sort()\n",
    "listlength_img = len(filelist_img)\n",
    "print('number .jpg:', listlength_img)\n",
    "\n",
    "# filelist = filelist[:3]\n",
    "listlength = len(filelist)\n",
    "print('\\nThere are %d lake files to process...' % listlength)\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "def time_string(secs):\n",
    "    m, s = divmod(secs, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    d, h = divmod(h, 24)\n",
    "    return '%d days, %d hrs, %d mins, %d secs' % (d, h, m, s)\n",
    "plt.close('all')\n",
    "\n",
    "tstart = time.time()\n",
    "idone = 0\n",
    "for i, fn in enumerate(filelist):\n",
    "\n",
    "    fn_plot = fn.replace('.h5', '_imagery.jpg')\n",
    "    fn_imagery = fn.replace('/lakes_data/', '/imagery/').replace('.h5', '_imagery.tif')\n",
    "\n",
    "    if (not os.path.isfile(fn_plot)) or replace_existing:\n",
    "        \n",
    "        tnow = time.time()\n",
    "        selapsed = tnow - tstart\n",
    "        elapsed = time_string(selapsed)\n",
    "        if idone > 0:\n",
    "            sremaining = (listlength-i) / idone * selapsed\n",
    "            remaining = time_string(sremaining)\n",
    "        else:\n",
    "            remaining = 'undefined'\n",
    "        ns = int(np.round((i+1)/listlength*100))\n",
    "        print('\\n[' + '#'*ns + '.'*(100-ns) + ']\\n')\n",
    "        print('%i / %i lakes processed (%i this run)' % (i+1, listlength, idone+1))\n",
    "        print('%s elapsed' % elapsed)\n",
    "        print('%s remaining' % remaining)\n",
    "        print('current file: %s' % fn_plot.split('/')[-1])\n",
    "        if i < (listlength-1):\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        idone += 1\n",
    "\n",
    "        settings = {\n",
    "            're_download': True,     # True\n",
    "            'img_aspect': 1.0,        # 1.0\n",
    "            'days_buffer': 5,         # 5\n",
    "            'max_cloud_scene': 20,    # 20               (make this smaller, more like 30?)\n",
    "            'max_cloud_mask': 40,     # 50               (make this smaller, more like 30?)\n",
    "            'n_imgs_mosaic': 5,       # 7                (put to one for checking code)\n",
    "            'mosaic_method': 'mean',  # mean             (alternatively \"median\" or \"mosaic\")\n",
    "            'scale_out': 10,          # 10\n",
    "            'gamma_value': 1.0,       # 1.0\n",
    "            'xlm': [None, None],      # [None, None]     (keep at None unless adjusting for a particular plot)\n",
    "            'ylm': [None, None],      # [None, None]     (keep at None unless adjusting for a particular plot)\n",
    "            'return_fig': False       # False\n",
    "        }\n",
    "    \n",
    "        fig = plt.figure(figsize=[10,4.35])\n",
    "        gs = fig.add_gridspec(ncols=9, nrows=1)\n",
    "        \n",
    "        axs = []\n",
    "        axs.append(fig.add_subplot(gs[0, :4])) \n",
    "        axs.append(fig.add_subplot(gs[0, 4:]))\n",
    "\n",
    "        try:\n",
    "            lk = dictobj(read_melt_lake_h5(fn))\n",
    "            \n",
    "            plot_IS2_imagery(fn=fn, imagery_filename=fn_imagery, **settings, axes=axs)\n",
    "            fig.tight_layout(pad=0.3, h_pad=0.3, w_pad=0.4)\n",
    "        except:\n",
    "            axs[0].text(0.5, 0.5, 'plotting error', fontsize=12, ha='center', va='center', transform=axs[0].transAxes)\n",
    "            traceback.print_exc()\n",
    "    \n",
    "        plt.close(fig)\n",
    "        fig.savefig(fn_plot, dpi=300)\n",
    "\n",
    "# show plots\n",
    "# for i, fn in enumerate(filelist):\n",
    "#     display(Image(fn.replace('.h5', '_imagery.jpg')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeicelakes-env",
   "language": "python",
   "name": "eeicelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
